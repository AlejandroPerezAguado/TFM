{"cells":[{"cell_type":"markdown","metadata":{"id":"e_Fo89pZAHZw"},"source":["Enfoque del dataset de Dipromats basado en algoritmos clásicos de aprendizaje automático"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6491,"status":"ok","timestamp":1688650746130,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"6p1Sno_M_bhI","outputId":"0cd65f0d-9b5d-489c-c620-653c90b1d962"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: textaugment in /usr/local/lib/python3.10/dist-packages (1.3.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textaugment) (3.8.1)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from textaugment) (4.3.1)\n","Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (from textaugment) (0.17.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from textaugment) (1.22.4)\n","Requirement already satisfied: googletrans in /usr/local/lib/python3.10/dist-packages (from textaugment) (3.0.0)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->textaugment) (1.10.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->textaugment) (6.3.0)\n","Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans->textaugment) (0.13.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (2023.5.7)\n","Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (2023.1.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (1.3.0)\n","Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (3.0.4)\n","Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (2.10)\n","Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (1.5.0)\n","Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (0.9.1)\n","Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (0.9.0)\n","Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (3.2.0)\n","Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (5.2.0)\n","Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (3.0.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (4.65.0)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}],"source":["### IMPORTS ###\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","#from keras.utils import to_categorical\n","\n","!pip install textaugment\n","from textaugment import EDA\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from nltk.stem.porter import PorterStemmer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","import re\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"markdown","metadata":{"id":"WbjNq11JgYqU"},"source":["# Cargar Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21609,"status":"ok","timestamp":1688650767729,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"2ohgLOFB5LmD","outputId":"6d24b09b-70b1-40ee-d9f0-4d65947445f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","PATH = \"/content/drive/My Drive/TFM/Data/Dipromats/\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1688650767730,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"X8Yhoi3y5MKQ","outputId":"c0d69456-9daf-4b15-a43d-cf8335db9fd2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño conjunto de Entrenamiento: 839\n","Tamaño conjunto de Validación: 122\n","Tamaño conjunto de Evaluación: 238\n"]}],"source":["### PARTICIÓN ###\n","df = pd.read_json(PATH + \"train_es.json\",  encoding='utf-8', encoding_errors='ignore')\n","df = df[['text', 'label_task2']]\n","df = df.rename(columns={\"text\": \"Text\", \"label_task2\": \"Label\"})\n","df.fillna(\" \", inplace=True)\n","\n","df = df[df.Label != \"\"]\n","df = df[df.Label != \"Group 4\"]\n","\n","X_train = df['Text']\n","y_train = df['Label']\n","\n","X_train, X_aux, y_train, y_aux = train_test_split(X_train, y_train, test_size=0.3, random_state=55)\n","X_val, X_test, y_val, y_test = train_test_split(X_aux, y_aux, test_size=0.66, random_state=55)\n","\n","X_train = X_train.reset_index(drop=True)\n","y_train = y_train.reset_index(drop=True)\n","\n","print('Tamaño conjunto de Entrenamiento:', len(X_train))\n","print('Tamaño conjunto de Validación:', len(X_val))\n","print('Tamaño conjunto de Evaluación:', len(X_test))"]},{"cell_type":"markdown","metadata":{"id":"Y2cga-nDjtlO"},"source":["# Limpieza y Representación de Textos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"badgpuYujv9M"},"outputs":[],"source":["### LIMPIEZA DE TEXTOS ###\n","stopwords_es = stopwords.words(\"spanish\")\n","def clean_text(text):\n","    # transformar a minúscula\n","    text=str(text).lower()\n","    # tokenizar\n","    tokens=word_tokenize(text)\n","    # borrar stopwords\n","    tokens = [word for word in tokens if word not in stopwords_es]\n","    # usar los stems\n","    tokens = [PorterStemmer().stem(word) for word in tokens]\n","    # eliminamos las palabras con menos de 2 caráceres\n","    # ignoramos cualquier palabra que contenga un digito o un símbolo especial\n","    min_length = 2\n","    p = re.compile('^[a-zA-Z]+$');\n","    filtered_tokens=[]\n","    for token in tokens:\n","        if len(token)>=min_length and p.match(token):\n","            filtered_tokens.append(token)\n","\n","    return filtered_tokens"]},{"cell_type":"markdown","metadata":{"id":"OuwFj0heTKW6"},"source":["# Data Augmentation"]},{"cell_type":"code","source":["t = EDA()\n","\n","for i in range(839):\n","    text = str(X_train[i])\n","\n","    #new_text = t.synonym_replacement(text)\n","    #X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","    #y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","\n","    new_text = t.random_swap(text)\n","    X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","    y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","\n","    new_text = t.random_deletion(text, p=0.2)\n","    X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","    y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","\n","print('Tamaño conjunto de Entrenamiento:', len(X_train))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BxRTTp8TCwED","executionInfo":{"status":"ok","timestamp":1688650769079,"user_tz":-120,"elapsed":1358,"user":{"displayName":"Alejandro","userId":"04676374624080257736"}},"outputId":"c5cd030c-ca7d-4fa7-971b-1d9b0b34fce2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-5d3d930493b0>:11: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","<ipython-input-5-5d3d930493b0>:12: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","<ipython-input-5-5d3d930493b0>:15: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","<ipython-input-5-5d3d930493b0>:16: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Tamaño conjunto de Entrenamiento: 2517\n"]}]},{"cell_type":"markdown","metadata":{"id":"eAc3QSGcgcdG"},"source":["# Label Encoding"]},{"cell_type":"code","source":["y_train = y_train.replace([\"Group 1\", \"Group 2\", \"Group 3\"], [0, 1, 2])\n","\n","y_test = y_test.replace([\"Group 1\", \"Group 2\", \"Group 3\"], [0, 1, 2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rbdIfXpKYBrf","executionInfo":{"status":"ok","timestamp":1688650769080,"user_tz":-120,"elapsed":11,"user":{"displayName":"Alejandro","userId":"04676374624080257736"}},"outputId":"3604f9ac-936c-4075-9f58-907dee98547a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0       0\n","1       1\n","2       1\n","3       0\n","4       2\n","       ..\n","2512    2\n","2513    1\n","2514    1\n","2515    2\n","2516    2\n","Length: 2517, dtype: int64\n"]}]},{"cell_type":"markdown","metadata":{"id":"BB2aQGIftSjU"},"source":["# Bolsa de Palabras"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13450,"status":"ok","timestamp":1688650782523,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"2LyyJkH4tU9K","outputId":"715a013e-5f17-43dc-c786-0e299614c3fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño del vocabulario:  4249\n"]}],"source":["### BOLSA DE PALABRAS ###\n","X_train = X_train.tolist()\n","X_test = X_test.tolist()\n","\n","# entrenamos un modelo de bolsa de palabras\n","bow = CountVectorizer(analyzer=clean_text).fit(X_train)\n","# transformamos el conjunto de entrenamiento a bolsa de palabras\n","X_train_bow = bow.transform(X_train)\n","# transformamos el conjunto de evaluación a bolsa de palabras\n","X_test_bow = bow.transform(X_test)\n","\n","print(\"Tamaño del vocabulario: \", len(bow.vocabulary_))"]},{"cell_type":"markdown","metadata":{"id":"ESXpf0ZAvLAq"},"source":["# TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6kuJq4avM_g"},"outputs":[],"source":["### TF-IDF ###\n","# entrenamos un modelo tf-idf\n","tfidf_transformer = TfidfTransformer().fit(X_train_bow)\n","# transformamos el conjunto de entrenamiento\n","X_train_tfidf = tfidf_transformer.transform(X_train_bow)\n","# transformamos el conjunto de entrenamiento\n","X_test_tfidf = tfidf_transformer.transform(X_test_bow)"]},{"cell_type":"markdown","metadata":{"id":"24Zza6O4wC4q"},"source":["# Clasificación Clásica\n","Se crea un pipeline que ejecuta una secuencia de procesos:\n","\n","\n","1.   La representación de los textos en bolsa de palabras (CountVectorizer), que recibe como entrada los textos, y se les aplica dentro del CountVectorizer la función clean_text para limpiarlos y reducir el ruido.\n","2.   La representación en tf-idf (TfidfTransformer), recibe como entrada la salida del proceso 1, y produce los vectores tf-idf.\n","3. El clasificador SVC, Logistic Regression o Random Forest Clasiffier."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":131499,"status":"ok","timestamp":1688650913999,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"-vlh4ZNrwFcH","outputId":"1518cf65-7a9a-4bed-c75d-eff27014bb52"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 4 folds for each of 8 candidates, totalling 32 fits\n","Los mejores parámetros son : {'svm__C': 1, 'svm__gamma': 1, 'svm__kernel': 'rbf'}\n","Mejor accuracy: 0.988\n","Pipeline(steps=[('bow',\n","                 CountVectorizer(analyzer=<function clean_text at 0x7f9d4e2dc430>)),\n","                ('tf', TfidfTransformer()), ('svm', SVC(C=1, gamma=1))])\n","              precision    recall  f1-score   support\n","\n","           0       0.85      0.46      0.60        63\n","           1       0.71      0.95      0.81       109\n","           2       0.60      0.52      0.55        66\n","\n","    accuracy                           0.70       238\n","   macro avg       0.72      0.64      0.65       238\n","weighted avg       0.72      0.70      0.68       238\n","\n"]}],"source":["### PIPELINE SVM ###\n","pipeline = Pipeline([\n","    ('bow', CountVectorizer(analyzer=clean_text)),\n","    ('tf', TfidfTransformer()),\n","    ('svm', SVC()),\n","])\n","\n","# Parámetros para el algoritmo SVM\n","grid_params_svm = [{'svm__kernel': ['linear', 'rbf'],\n","                    'svm__C': [0.1, 1], # [0.1, 1, 10, 100, 1000]\n","                    'svm__gamma':  [1, 0.1] # [1, 0.1, 0.01, 0.001, 0.0001]\n","                    }]\n","gs = GridSearchCV(pipeline, param_grid=grid_params_svm,\n","                  scoring='accuracy', cv=4, verbose = 1)\n","\n","# entrenamos el grid\n","gs.fit(X_train, y_train)\n","print('Los mejores parámetros son : %s' % gs.best_params_)\n","print('Mejor accuracy: %.3f' % gs.best_score_)\n","print(gs.best_estimator_)\n","\n","best_svm = gs.best_estimator_\n","predictions = best_svm.predict(X_test)\n","print(classification_report(y_test, predictions))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61758,"status":"ok","timestamp":1688650975733,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"nvM8_10-ajC-","outputId":"acd5875a-bb7d-47e7-993c-134236245bf8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 4 candidates, totalling 20 fits\n","Los mejores parámetros son : {'lr__C': 1.0, 'lr__penalty': 'l2', 'lr__solver': 'liblinear'}\n","Mejor accuracy: 0.857\n","Pipeline(steps=[('bow',\n","                 CountVectorizer(analyzer=<function clean_text at 0x7f9d4e2dc430>)),\n","                ('tf', TfidfTransformer()),\n","                ('lr', LogisticRegression(random_state=0, solver='liblinear'))])\n","              precision    recall  f1-score   support\n","\n","           0       0.79      0.48      0.59        63\n","           1       0.72      0.97      0.82       109\n","           2       0.58      0.45      0.51        66\n","\n","    accuracy                           0.70       238\n","   macro avg       0.69      0.63      0.64       238\n","weighted avg       0.70      0.70      0.68       238\n","\n"]}],"source":["### PIPELINE LOGISTIC REGRESSION ###\n","pipeline2 = Pipeline([\n","    ('bow', CountVectorizer(analyzer=clean_text)),\n","    ('tf', TfidfTransformer()),\n","    ('lr', LogisticRegression(random_state=0)),\n","])\n","\n","# Parámetros para el algoritmo Logistic Regression\n","grid_params_lr = [{'lr__penalty': ['l1', 'l2'],\n","                    'lr__C': [1.0, 0.5],\n","                    'lr__solver':  ['liblinear']\n","                    }]\n","gs2 = GridSearchCV(pipeline2, param_grid=grid_params_lr,\n","                  scoring='accuracy', cv=5, verbose = 1)\n","\n","# entrenamos el grid\n","gs2.fit(X_train, y_train)\n","print('Los mejores parámetros son : %s' % gs2.best_params_)\n","print('Mejor accuracy: %.3f' % gs2.best_score_)\n","print(gs2.best_estimator_)\n","\n","best_svm = gs2.best_estimator_\n","predictions = best_svm.predict(X_test)\n","print( classification_report(y_test, predictions))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64943,"status":"ok","timestamp":1688651040643,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"CMuW5K410K-_","outputId":"bc257934-a673-4066-f4fd-94c14c9e387a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 4 candidates, totalling 20 fits\n","Los mejores parámetros son : {'rfc__criterion': 'gini', 'rfc__max_depth': 10, 'rfc__min_samples_split': 10}\n","Mejor accuracy: 0.566\n","Pipeline(steps=[('bow',\n","                 CountVectorizer(analyzer=<function clean_text at 0x7f9d4e2dc430>)),\n","                ('tf', TfidfTransformer()),\n","                ('rfc',\n","                 RandomForestClassifier(max_depth=10, min_samples_split=10,\n","                                        random_state=0))])\n","              precision    recall  f1-score   support\n","\n","           0       0.83      0.08      0.14        63\n","           1       0.50      1.00      0.67       109\n","           2       0.93      0.21      0.35        66\n","\n","    accuracy                           0.54       238\n","   macro avg       0.76      0.43      0.39       238\n","weighted avg       0.71      0.54      0.44       238\n","\n"]}],"source":["### PIPELINE RANDOM FOREST ###\n","pipeline3 = Pipeline([\n","    ('bow', CountVectorizer(analyzer=clean_text)),\n","    ('tf', TfidfTransformer()),\n","    ('rfc', RandomForestClassifier(random_state=0)),\n","])\n","\n","# Parámetros para el algoritmo Random Forest\n","grid_params_rfc = [{'rfc__criterion': ['gini', 'entropy'],\n","                    'rfc__max_depth': [9, 10],\n","                    'rfc__min_samples_split':  [10]\n","                    }]\n","gs3 = GridSearchCV(pipeline3, param_grid=grid_params_rfc,\n","                  scoring='accuracy', cv=5, verbose = 1)\n","\n","# entrenamos el grid\n","gs3.fit(X_train, y_train)\n","print('Los mejores parámetros son : %s' % gs3.best_params_)\n","print('Mejor accuracy: %.3f' % gs3.best_score_)\n","print(gs3.best_estimator_)\n","\n","best_svm = gs3.best_estimator_\n","predictions = best_svm.predict(X_test)\n","print( classification_report(y_test, predictions))"]}],"metadata":{"colab":{"provenance":[],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}