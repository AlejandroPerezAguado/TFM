{"cells":[{"cell_type":"markdown","metadata":{"id":"e_Fo89pZAHZw"},"source":["Enfoque del dataset de Dipromats basado en algoritmos clásicos de aprendizaje automático"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15804,"status":"ok","timestamp":1688680939140,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"6p1Sno_M_bhI","outputId":"f0ae8ed3-643f-4402-ca5a-3014374bd333"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting textaugment\n","  Downloading textaugment-1.3.4-py3-none-any.whl (16 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textaugment) (3.8.1)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from textaugment) (4.3.1)\n","Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (from textaugment) (0.17.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from textaugment) (1.22.4)\n","Collecting googletrans (from textaugment)\n","  Downloading googletrans-3.0.0.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->textaugment) (1.10.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->textaugment) (6.3.0)\n","Collecting httpx==0.13.3 (from googletrans->textaugment)\n","  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (2023.5.7)\n","Collecting hstspreload (from httpx==0.13.3->googletrans->textaugment)\n","  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (1.3.0)\n","Collecting chardet==3.* (from httpx==0.13.3->googletrans->textaugment)\n","  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans->textaugment)\n","  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans->textaugment)\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n","Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans->textaugment)\n","  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment)\n","  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment)\n","  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment)\n","  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n","Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment)\n","  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (4.65.0)\n","Building wheels for collected packages: googletrans\n","  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15721 sha256=e8af514da5c168aa078191d48aca19ccfc14bf27fea025f6dab59f7998f2faba\n","  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\n","Successfully built googletrans\n","Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans, textaugment\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 4.0.0\n","    Uninstalling chardet-4.0.0:\n","      Successfully uninstalled chardet-4.0.0\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.4\n","    Uninstalling idna-3.4:\n","      Successfully uninstalled idna-3.4\n","Successfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0 textaugment-1.3.4\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}],"source":["### IMPORTS ###\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","#from keras.utils import to_categorical\n","\n","!pip install textaugment\n","from textaugment import EDA\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from nltk.stem.porter import PorterStemmer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","import re\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"markdown","metadata":{"id":"WbjNq11JgYqU"},"source":["# Cargar Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":73545,"status":"ok","timestamp":1688681012673,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"2ohgLOFB5LmD","outputId":"ea100f80-7794-4c77-f45d-3acd92859459"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","PATH = \"/content/drive/My Drive/TFM/Data/Dipromats/\""]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1691,"status":"ok","timestamp":1688681014353,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"X8Yhoi3y5MKQ","outputId":"c6b6ee26-1d71-4750-da3d-5a22f2ffcbb8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño conjunto de Entrenamiento: 1379\n","Tamaño conjunto de Validación: 200\n","Tamaño conjunto de Evaluación: 391\n"]}],"source":["### PARTICIÓN ###\n","df = pd.read_json(PATH + \"train_en.json\",  encoding='utf-8', encoding_errors='ignore')\n","df = df[['text', 'label_task2']]\n","df = df.rename(columns={\"text\": \"Text\", \"label_task2\": \"Label\"})\n","df.fillna(\" \", inplace=True)\n","\n","df = df[df.Label != \"\"]\n","df = df[df.Label != \"Group 4\"]\n","\n","X_train = df['Text']\n","y_train = df['Label']\n","\n","X_train, X_aux, y_train, y_aux = train_test_split(X_train, y_train, test_size=0.3, random_state=55)\n","X_val, X_test, y_val, y_test = train_test_split(X_aux, y_aux, test_size=0.66, random_state=55)\n","\n","X_train = X_train.reset_index(drop=True)\n","y_train = y_train.reset_index(drop=True)\n","\n","print('Tamaño conjunto de Entrenamiento:', len(X_train))\n","print('Tamaño conjunto de Validación:', len(X_val))\n","print('Tamaño conjunto de Evaluación:', len(X_test))"]},{"cell_type":"markdown","metadata":{"id":"Y2cga-nDjtlO"},"source":["# Limpieza y Representación de Textos"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"badgpuYujv9M","executionInfo":{"status":"ok","timestamp":1688681014353,"user_tz":-120,"elapsed":5,"user":{"displayName":"Alejandro","userId":"04676374624080257736"}}},"outputs":[],"source":["### LIMPIEZA DE TEXTOS ###\n","stopwords_es = stopwords.words(\"english\")\n","def clean_text(text):\n","    # transformar a minúscula\n","    text=str(text).lower()\n","    # tokenizar\n","    tokens=word_tokenize(text)\n","    # borrar stopwords\n","    tokens = [word for word in tokens if word not in stopwords_es]\n","    # usar los stems\n","    tokens = [PorterStemmer().stem(word) for word in tokens]\n","    # eliminamos las palabras con menos de 2 caráceres\n","    # ignoramos cualquier palabra que contenga un digito o un símbolo especial\n","    min_length = 2\n","    p = re.compile('^[a-zA-Z]+$');\n","    filtered_tokens=[]\n","    for token in tokens:\n","        if len(token)>=min_length and p.match(token):\n","            filtered_tokens.append(token)\n","\n","    return filtered_tokens"]},{"cell_type":"markdown","metadata":{"id":"OuwFj0heTKW6"},"source":["# Data Augmentation"]},{"cell_type":"code","source":["t = EDA()\n","\n","for i in range(839):\n","    text = str(X_train[i])\n","\n","    new_text = t.synonym_replacement(text)\n","    X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","    y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","\n","    new_text = t.random_swap(text)\n","    X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","    y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","\n","    new_text = t.random_deletion(text, p=0.2)\n","    X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","    y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","\n","print('Tamaño conjunto de Entrenamiento:', len(X_train))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BxRTTp8TCwED","executionInfo":{"status":"ok","timestamp":1688681018377,"user_tz":-120,"elapsed":4028,"user":{"displayName":"Alejandro","userId":"04676374624080257736"}},"outputId":"4313a356-1358-4baf-fcfc-358b9fbb0bab"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-c5d9b130afbf>:7: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","<ipython-input-5-c5d9b130afbf>:8: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","<ipython-input-5-c5d9b130afbf>:11: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","<ipython-input-5-c5d9b130afbf>:12: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","<ipython-input-5-c5d9b130afbf>:15: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","<ipython-input-5-c5d9b130afbf>:16: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Tamaño conjunto de Entrenamiento: 3896\n"]}]},{"cell_type":"markdown","metadata":{"id":"eAc3QSGcgcdG"},"source":["# Label Encoding"]},{"cell_type":"code","source":["y_train = y_train.replace([\"Group 1\", \"Group 2\", \"Group 3\"], [0, 1, 2])\n","\n","y_test = y_test.replace([\"Group 1\", \"Group 2\", \"Group 3\"], [0, 1, 2])"],"metadata":{"id":"rbdIfXpKYBrf","executionInfo":{"status":"ok","timestamp":1688681018378,"user_tz":-120,"elapsed":7,"user":{"displayName":"Alejandro","userId":"04676374624080257736"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BB2aQGIftSjU"},"source":["# Bolsa de Palabras"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33603,"status":"ok","timestamp":1688681051976,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"2LyyJkH4tU9K","outputId":"b6682b91-accd-4e29-91aa-68c03fa9a463"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño del vocabulario:  5141\n"]}],"source":["### BOLSA DE PALABRAS ###\n","X_train = X_train.tolist()\n","X_test = X_test.tolist()\n","\n","# entrenamos un modelo de bolsa de palabras\n","bow = CountVectorizer(analyzer=clean_text).fit(X_train)\n","# transformamos el conjunto de entrenamiento a bolsa de palabras\n","X_train_bow = bow.transform(X_train)\n","# transformamos el conjunto de evaluación a bolsa de palabras\n","X_test_bow = bow.transform(X_test)\n","\n","print(\"Tamaño del vocabulario: \", len(bow.vocabulary_))"]},{"cell_type":"markdown","metadata":{"id":"ESXpf0ZAvLAq"},"source":["# TF-IDF"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"A6kuJq4avM_g","executionInfo":{"status":"ok","timestamp":1688681051977,"user_tz":-120,"elapsed":8,"user":{"displayName":"Alejandro","userId":"04676374624080257736"}}},"outputs":[],"source":["### TF-IDF ###\n","# entrenamos un modelo tf-idf\n","tfidf_transformer = TfidfTransformer().fit(X_train_bow)\n","# transformamos el conjunto de entrenamiento\n","X_train_tfidf = tfidf_transformer.transform(X_train_bow)\n","# transformamos el conjunto de entrenamiento\n","X_test_tfidf = tfidf_transformer.transform(X_test_bow)"]},{"cell_type":"markdown","metadata":{"id":"24Zza6O4wC4q"},"source":["# Clasificación Clásica\n","Se crea un pipeline que ejecuta una secuencia de procesos:\n","\n","\n","1.   La representación de los textos en bolsa de palabras (CountVectorizer), que recibe como entrada los textos, y se les aplica dentro del CountVectorizer la función clean_text para limpiarlos y reducir el ruido.\n","2.   La representación en tf-idf (TfidfTransformer), recibe como entrada la salida del proceso 1, y produce los vectores tf-idf.\n","3. El clasificador SVC, Logistic Regression o Random Forest Clasiffier."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":246105,"status":"ok","timestamp":1688681298075,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"-vlh4ZNrwFcH","outputId":"5e574711-50b2-4854-8aa5-23d108cba15b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 4 folds for each of 8 candidates, totalling 32 fits\n","Los mejores parámetros son : {'svm__C': 1, 'svm__gamma': 1, 'svm__kernel': 'rbf'}\n","Mejor accuracy: 0.945\n","Pipeline(steps=[('bow',\n","                 CountVectorizer(analyzer=<function clean_text at 0x7f5652f74c10>)),\n","                ('tf', TfidfTransformer()), ('svm', SVC(C=1, gamma=1))])\n","              precision    recall  f1-score   support\n","\n","           0       0.74      0.76      0.75       114\n","           1       0.69      0.73      0.71       125\n","           2       0.68      0.63      0.65       152\n","\n","    accuracy                           0.70       391\n","   macro avg       0.70      0.71      0.70       391\n","weighted avg       0.70      0.70      0.70       391\n","\n"]}],"source":["### PIPELINE SVM ###\n","pipeline = Pipeline([\n","    ('bow', CountVectorizer(analyzer=clean_text)),\n","    ('tf', TfidfTransformer()),\n","    ('svm', SVC()),\n","])\n","\n","# Parámetros para el algoritmo SVM\n","grid_params_svm = [{'svm__kernel': ['linear', 'rbf'],\n","                    'svm__C': [0.1, 1], # [0.1, 1, 10, 100, 1000]\n","                    'svm__gamma':  [1, 0.1] # [1, 0.1, 0.01, 0.001, 0.0001]\n","                    }]\n","gs = GridSearchCV(pipeline, param_grid=grid_params_svm,\n","                  scoring='accuracy', cv=4, verbose = 1)\n","\n","# entrenamos el grid\n","gs.fit(X_train, y_train)\n","print('Los mejores parámetros son : %s' % gs.best_params_)\n","print('Mejor accuracy: %.3f' % gs.best_score_)\n","print(gs.best_estimator_)\n","\n","best_svm = gs.best_estimator_\n","predictions = best_svm.predict(X_test)\n","print(classification_report(y_test, predictions))"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":101520,"status":"ok","timestamp":1688681399549,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"nvM8_10-ajC-","outputId":"436291df-4108-4413-a095-2ce1ea987a59"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 4 candidates, totalling 20 fits\n","Los mejores parámetros son : {'lr__C': 1.0, 'lr__penalty': 'l2', 'lr__solver': 'liblinear'}\n","Mejor accuracy: 0.832\n","Pipeline(steps=[('bow',\n","                 CountVectorizer(analyzer=<function clean_text at 0x7f5652f74c10>)),\n","                ('tf', TfidfTransformer()),\n","                ('lr', LogisticRegression(random_state=0, solver='liblinear'))])\n","              precision    recall  f1-score   support\n","\n","           0       0.69      0.79      0.73       114\n","           1       0.67      0.74      0.70       125\n","           2       0.72      0.58      0.64       152\n","\n","    accuracy                           0.69       391\n","   macro avg       0.69      0.70      0.69       391\n","weighted avg       0.69      0.69      0.69       391\n","\n"]}],"source":["### PIPELINE LOGISTIC REGRESSION ###\n","pipeline2 = Pipeline([\n","    ('bow', CountVectorizer(analyzer=clean_text)),\n","    ('tf', TfidfTransformer()),\n","    ('lr', LogisticRegression(random_state=0)),\n","])\n","\n","# Parámetros para el algoritmo Logistic Regression\n","grid_params_lr = [{'lr__penalty': ['l1', 'l2'],\n","                    'lr__C': [1.0, 0.5],\n","                    'lr__solver':  ['liblinear']\n","                    }]\n","gs2 = GridSearchCV(pipeline2, param_grid=grid_params_lr,\n","                  scoring='accuracy', cv=5, verbose = 1)\n","\n","# entrenamos el grid\n","gs2.fit(X_train, y_train)\n","print('Los mejores parámetros son : %s' % gs2.best_params_)\n","print('Mejor accuracy: %.3f' % gs2.best_score_)\n","print(gs2.best_estimator_)\n","\n","best_svm = gs2.best_estimator_\n","predictions = best_svm.predict(X_test)\n","print( classification_report(y_test, predictions))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":105575,"status":"ok","timestamp":1688681505100,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"CMuW5K410K-_","outputId":"0c6c5976-079d-4800-a06a-c940b159a781"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 4 candidates, totalling 20 fits\n","Los mejores parámetros son : {'rfc__criterion': 'entropy', 'rfc__max_depth': 10, 'rfc__min_samples_split': 10}\n","Mejor accuracy: 0.730\n","Pipeline(steps=[('bow',\n","                 CountVectorizer(analyzer=<function clean_text at 0x7f5652f74c10>)),\n","                ('tf', TfidfTransformer()),\n","                ('rfc',\n","                 RandomForestClassifier(criterion='entropy', max_depth=10,\n","                                        min_samples_split=10,\n","                                        random_state=0))])\n","              precision    recall  f1-score   support\n","\n","           0       0.73      0.46      0.57       114\n","           1       0.60      0.72      0.66       125\n","           2       0.55      0.61      0.58       152\n","\n","    accuracy                           0.60       391\n","   macro avg       0.63      0.60      0.60       391\n","weighted avg       0.62      0.60      0.60       391\n","\n"]}],"source":["### PIPELINE RANDOM FOREST ###\n","pipeline3 = Pipeline([\n","    ('bow', CountVectorizer(analyzer=clean_text)),\n","    ('tf', TfidfTransformer()),\n","    ('rfc', RandomForestClassifier(random_state=0)),\n","])\n","\n","# Parámetros para el algoritmo Random Forest\n","grid_params_rfc = [{'rfc__criterion': ['gini', 'entropy'],\n","                    'rfc__max_depth': [9, 10],\n","                    'rfc__min_samples_split':  [10]\n","                    }]\n","gs3 = GridSearchCV(pipeline3, param_grid=grid_params_rfc,\n","                  scoring='accuracy', cv=5, verbose = 1)\n","\n","# entrenamos el grid\n","gs3.fit(X_train, y_train)\n","print('Los mejores parámetros son : %s' % gs3.best_params_)\n","print('Mejor accuracy: %.3f' % gs3.best_score_)\n","print(gs3.best_estimator_)\n","\n","best_svm = gs3.best_estimator_\n","predictions = best_svm.predict(X_test)\n","print( classification_report(y_test, predictions))"]}],"metadata":{"colab":{"provenance":[],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}