{"cells":[{"cell_type":"markdown","metadata":{"id":"e_Fo89pZAHZw"},"source":["Enfoque del dataset de Dipromats basado en algoritmos clásicos de aprendizaje automático"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12601,"status":"ok","timestamp":1688593328640,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"6p1Sno_M_bhI","outputId":"36cd8985-9890-476e-b314-97211e3d69bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting textaugment\n","  Downloading textaugment-1.3.4-py3-none-any.whl (16 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textaugment) (3.8.1)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from textaugment) (4.3.1)\n","Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (from textaugment) (0.17.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from textaugment) (1.22.4)\n","Collecting googletrans (from textaugment)\n","  Downloading googletrans-3.0.0.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->textaugment) (1.10.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->textaugment) (6.3.0)\n","Collecting httpx==0.13.3 (from googletrans->textaugment)\n","  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (2023.5.7)\n","Collecting hstspreload (from httpx==0.13.3->googletrans->textaugment)\n","  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans->textaugment) (1.3.0)\n","Collecting chardet==3.* (from httpx==0.13.3->googletrans->textaugment)\n","  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans->textaugment)\n","  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans->textaugment)\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n","Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans->textaugment)\n","  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment)\n","  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment)\n","  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment)\n","  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n","Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment)\n","  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (4.65.0)\n","Building wheels for collected packages: googletrans\n","  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15721 sha256=b0620565f9fa170d82a6ac0ad02cacbe64a074a4268d173ca41cf38396259b58\n","  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\n","Successfully built googletrans\n","Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans, textaugment\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 4.0.0\n","    Uninstalling chardet-4.0.0:\n","      Successfully uninstalled chardet-4.0.0\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.4\n","    Uninstalling idna-3.4:\n","      Successfully uninstalled idna-3.4\n","Successfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0 textaugment-1.3.4\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}],"source":["### IMPORTS ###\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","#from keras.utils import to_categorical\n","\n","!pip install textaugment\n","from textaugment import EDA\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from nltk.stem.porter import PorterStemmer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","import re\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"markdown","metadata":{"id":"WbjNq11JgYqU"},"source":["# Cargar Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19264,"status":"ok","timestamp":1688593347891,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"2ohgLOFB5LmD","outputId":"e3e30d84-4617-4158-d92d-beac2db5781d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","PATH = \"/content/drive/My Drive/TFM/Data/Dipromats/\""]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2250,"status":"ok","timestamp":1688593350112,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"X8Yhoi3y5MKQ","outputId":"06c655ad-8bd8-4236-84f3-95cb037185b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño conjunto de Entrenamiento: 4896\n","Tamaño conjunto de Evaluación: 1224\n"]}],"source":["### PARTICIÓN ###\n","df = pd.read_json(PATH + \"train_es.json\",  encoding='utf-8', encoding_errors='ignore')\n","df = df[['text', 'label_task1']]\n","df = df.rename(columns={\"text\": \"Text\", \"label_task1\": \"Label\"})\n","df.fillna(\" \", inplace=True)\n","\n","X_train = df['Text']\n","y_train = df['Label']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=55, stratify=y_train)\n","\n","X_train = X_train.reset_index(drop=True)\n","y_train = y_train.reset_index(drop=True)\n","\n","print('Tamaño conjunto de Entrenamiento:', len(X_train))\n","print('Tamaño conjunto de Evaluación:', len(X_test))"]},{"cell_type":"markdown","metadata":{"id":"Y2cga-nDjtlO"},"source":["# Limpieza y Representación de Textos"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"badgpuYujv9M","executionInfo":{"status":"ok","timestamp":1688593350116,"user_tz":-120,"elapsed":20,"user":{"displayName":"Alejandro","userId":"04676374624080257736"}}},"outputs":[],"source":["### LIMPIEZA DE TEXTOS ###\n","stopwords_es = stopwords.words(\"spanish\")\n","def clean_text(text):\n","    # transformar a minúscula\n","    text=str(text).lower()\n","    # tokenizar\n","    tokens=word_tokenize(text)\n","    # borrar stopwords\n","    tokens = [word for word in tokens if word not in stopwords_es]\n","    # usar los stems\n","    tokens = [PorterStemmer().stem(word) for word in tokens]\n","    # eliminamos las palabras con menos de 2 caráceres\n","    # ignoramos cualquier palabra que contenga un digito o un símbolo especial\n","    min_length = 2\n","    p = re.compile('^[a-zA-Z]+$');\n","    filtered_tokens=[]\n","    for token in tokens:\n","        if len(token)>=min_length and p.match(token):\n","            filtered_tokens.append(token)\n","\n","    return filtered_tokens"]},{"cell_type":"markdown","metadata":{"id":"OuwFj0heTKW6"},"source":["# Data Augmentation"]},{"cell_type":"code","source":["t = EDA()\n","\n","x = 0\n","for i in range(2136):\n","    text = str(X_train[i])\n","\n","    #new_text = t.synonym_replacement(text)\n","    #X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","    #y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","\n","    if x == 0:\n","        new_text = t.random_swap(text)\n","        X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","        y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","    else:\n","        new_text = t.random_deletion(text, p=0.2)\n","        X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","        y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","\n","    x = x + 1\n","    if x == 2:\n","        x = 0\n","\n","print('Tamaño conjunto de Entrenamiento:', len(X_train))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BxRTTp8TCwED","executionInfo":{"status":"ok","timestamp":1688593351814,"user_tz":-120,"elapsed":1713,"user":{"displayName":"Alejandro","userId":"04676374624080257736"}},"outputId":"9c9b9435-64e6-4449-896a-c9db30c2d5e5"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-a646eb4e07ef>:13: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","<ipython-input-5-a646eb4e07ef>:14: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n","<ipython-input-5-a646eb4e07ef>:17: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  X_train = X_train.append(pd.Series([new_text]), ignore_index=True)\n","<ipython-input-5-a646eb4e07ef>:18: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  y_train = y_train.append(pd.Series([y_train[i]]), ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Tamaño conjunto de Entrenamiento: 7032\n"]}]},{"cell_type":"markdown","metadata":{"id":"eAc3QSGcgcdG"},"source":["# Label Encoding"]},{"cell_type":"markdown","metadata":{"id":"MFQRNM8cnQsV"},"source":["No es necesario hacer un encoding ya que viene etiquetado de manera numérica"]},{"cell_type":"markdown","metadata":{"id":"BB2aQGIftSjU"},"source":["# Bolsa de Palabras"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30633,"status":"ok","timestamp":1688593382439,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"2LyyJkH4tU9K","outputId":"cfd744c2-3871-4de4-fedc-b37dba0a3959"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño del vocabulario:  11172\n"]}],"source":["### BOLSA DE PALABRAS ###\n","X_train = X_train.tolist()\n","X_test = X_test.tolist()\n","\n","# entrenamos un modelo de bolsa de palabras\n","bow = CountVectorizer(analyzer=clean_text).fit(X_train)\n","# transformamos el conjunto de entrenamiento a bolsa de palabras\n","X_train_bow = bow.transform(X_train)\n","# transformamos el conjunto de evaluación a bolsa de palabras\n","X_test_bow = bow.transform(X_test)\n","\n","print(\"Tamaño del vocabulario: \", len(bow.vocabulary_))"]},{"cell_type":"markdown","metadata":{"id":"ESXpf0ZAvLAq"},"source":["# TF-IDF"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"A6kuJq4avM_g","executionInfo":{"status":"ok","timestamp":1688593382440,"user_tz":-120,"elapsed":8,"user":{"displayName":"Alejandro","userId":"04676374624080257736"}}},"outputs":[],"source":["### TF-IDF ###\n","# entrenamos un modelo tf-idf\n","tfidf_transformer = TfidfTransformer().fit(X_train_bow)\n","# transformamos el conjunto de entrenamiento\n","X_train_tfidf = tfidf_transformer.transform(X_train_bow)\n","# transformamos el conjunto de entrenamiento\n","X_test_tfidf = tfidf_transformer.transform(X_test_bow)"]},{"cell_type":"markdown","metadata":{"id":"24Zza6O4wC4q"},"source":["# Clasificación Clásica\n","Se crea un pipeline que ejecuta una secuencia de procesos:\n","\n","\n","1.   La representación de los textos en bolsa de palabras (CountVectorizer), que recibe como entrada los textos, y se les aplica dentro del CountVectorizer la función clean_text para limpiarlos y reducir el ruido.\n","2.   La representación en tf-idf (TfidfTransformer), recibe como entrada la salida del proceso 1, y produce los vectores tf-idf.\n","3. El clasificador SVC, Logistic Regression o Random Forest Clasiffier."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":423157,"status":"ok","timestamp":1688593805591,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"-vlh4ZNrwFcH","outputId":"2552f9aa-c7a2-4bca-96c6-5a5d0ca18a29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 4 folds for each of 8 candidates, totalling 32 fits\n","Los mejores parámetros son : {'svm__C': 1, 'svm__gamma': 1, 'svm__kernel': 'rbf'}\n","Mejor accuracy: 0.921\n","Pipeline(steps=[('bow',\n","                 CountVectorizer(analyzer=<function clean_text at 0x7faa72c383a0>)),\n","                ('tf', TfidfTransformer()), ('svm', SVC(C=1, gamma=1))])\n","              precision    recall  f1-score   support\n","\n","       False       0.85      0.99      0.92       984\n","        True       0.85      0.31      0.46       240\n","\n","    accuracy                           0.85      1224\n","   macro avg       0.85      0.65      0.69      1224\n","weighted avg       0.85      0.85      0.83      1224\n","\n"]}],"source":["### PIPELINE SVM ###\n","pipeline = Pipeline([\n","    ('bow', CountVectorizer(analyzer=clean_text)),\n","    ('tf', TfidfTransformer()),\n","    ('svm', SVC()),\n","])\n","\n","# Parámetros para el algoritmo SVM\n","grid_params_svm = [{'svm__kernel': ['linear', 'rbf'],\n","                    'svm__C': [0.1, 1], # [0.1, 1, 10, 100, 1000]\n","                    'svm__gamma':  [1, 0.1] # [1, 0.1, 0.01, 0.001, 0.0001]\n","                    }]\n","gs = GridSearchCV(pipeline, param_grid=grid_params_svm,\n","                  scoring='accuracy', cv=4, verbose = 1)\n","\n","# entrenamos el grid\n","gs.fit(X_train, y_train)\n","print('Los mejores parámetros son : %s' % gs.best_params_)\n","print('Mejor accuracy: %.3f' % gs.best_score_)\n","print(gs.best_estimator_)\n","\n","best_svm = gs.best_estimator_\n","predictions = best_svm.predict(X_test)\n","print(classification_report(y_test, predictions))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":169809,"status":"ok","timestamp":1688593975372,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"nvM8_10-ajC-","outputId":"a31bd18e-4701-4498-a09c-40e363915806"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 4 candidates, totalling 20 fits\n","Los mejores parámetros son : {'lr__C': 1.0, 'lr__penalty': 'l2', 'lr__solver': 'liblinear'}\n","Mejor accuracy: 0.855\n","Pipeline(steps=[('bow',\n","                 CountVectorizer(analyzer=<function clean_text at 0x7faa72c383a0>)),\n","                ('tf', TfidfTransformer()),\n","                ('lr', LogisticRegression(random_state=0, solver='liblinear'))])\n","              precision    recall  f1-score   support\n","\n","       False       0.85      0.98      0.91       984\n","        True       0.81      0.27      0.40       240\n","\n","    accuracy                           0.84      1224\n","   macro avg       0.83      0.63      0.66      1224\n","weighted avg       0.84      0.84      0.81      1224\n","\n"]}],"source":["### PIPELINE LOGISTIC REGRESSION ###\n","pipeline2 = Pipeline([\n","    ('bow', CountVectorizer(analyzer=clean_text)),\n","    ('tf', TfidfTransformer()),\n","    ('lr', LogisticRegression(random_state=0)),\n","])\n","\n","# Parámetros para el algoritmo Logistic Regression\n","grid_params_lr = [{'lr__penalty': ['l1', 'l2'],\n","                    'lr__C': [1.0, 0.5],\n","                    'lr__solver':  ['liblinear']\n","                    }]\n","gs2 = GridSearchCV(pipeline2, param_grid=grid_params_lr,\n","                  scoring='accuracy', cv=5, verbose = 1)\n","\n","# entrenamos el grid\n","gs2.fit(X_train, y_train)\n","print('Los mejores parámetros son : %s' % gs2.best_params_)\n","print('Mejor accuracy: %.3f' % gs2.best_score_)\n","print(gs2.best_estimator_)\n","\n","best_svm = gs2.best_estimator_\n","predictions = best_svm.predict(X_test)\n","print( classification_report(y_test, predictions))"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":177416,"status":"ok","timestamp":1688594152728,"user":{"displayName":"Alejandro","userId":"04676374624080257736"},"user_tz":-120},"id":"CMuW5K410K-_","outputId":"e3f3296a-7ff5-4015-bc8c-61f6746c2702"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 4 candidates, totalling 20 fits\n","Los mejores parámetros son : {'rfc__criterion': 'gini', 'rfc__max_depth': 9, 'rfc__min_samples_split': 10}\n","Mejor accuracy: 0.806\n","Pipeline(steps=[('bow',\n","                 CountVectorizer(analyzer=<function clean_text at 0x7faa72c383a0>)),\n","                ('tf', TfidfTransformer()),\n","                ('rfc',\n","                 RandomForestClassifier(max_depth=9, min_samples_split=10,\n","                                        random_state=0))])\n","              precision    recall  f1-score   support\n","\n","       False       0.81      1.00      0.89       984\n","        True       1.00      0.01      0.02       240\n","\n","    accuracy                           0.81      1224\n","   macro avg       0.90      0.51      0.46      1224\n","weighted avg       0.84      0.81      0.72      1224\n","\n"]}],"source":["### PIPELINE RANDOM FOREST ###\n","pipeline3 = Pipeline([\n","    ('bow', CountVectorizer(analyzer=clean_text)),\n","    ('tf', TfidfTransformer()),\n","    ('rfc', RandomForestClassifier(random_state=0)),\n","])\n","\n","# Parámetros para el algoritmo Random Forest\n","grid_params_rfc = [{'rfc__criterion': ['gini', 'entropy'],\n","                    'rfc__max_depth': [9, 10],\n","                    'rfc__min_samples_split':  [10]\n","                    }]\n","gs3 = GridSearchCV(pipeline3, param_grid=grid_params_rfc,\n","                  scoring='accuracy', cv=5, verbose = 1)\n","\n","# entrenamos el grid\n","gs3.fit(X_train, y_train)\n","print('Los mejores parámetros son : %s' % gs3.best_params_)\n","print('Mejor accuracy: %.3f' % gs3.best_score_)\n","print(gs3.best_estimator_)\n","\n","best_svm = gs3.best_estimator_\n","predictions = best_svm.predict(X_test)\n","print( classification_report(y_test, predictions))"]}],"metadata":{"colab":{"provenance":[],"gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}